stages:
  - stage: cloud_db_integration
    pool:
      vmImage: 'ubuntu-latest'

    jobs:
      - job: bigquery_expectations
        timeoutInMinutes: 0 # Maximize the time that pipelines remain open (6 hours currently)
        variables:
          python.version: '3.8'

        strategy:
          matrix:
            expectations_cfe:
              test_script: 'tests/test_definitions/test_expectations_cfe.py'
            expectations:
              test_script: 'tests/test_definitions/test_expectations.py'
          maxParallel: 1

        steps:
          - task: UsePythonVersion@0
            inputs:
              versionSpec: '$(python.version)'
            displayName: 'Use Python $(python.version)'

          - bash: python -m pip install --upgrade pip==20.2.4
            displayName: 'Update pip'

          - script: |
              pip install -r requirements-dev.txt

            displayName: 'Install dependencies'

          - task: DownloadSecureFile@1
            name: gcp_authkey
            displayName: 'Download Google Service Account'
            inputs:
              secureFile: 'superconductive-service-acct.json'
              retryCount: '2'

          - script: |
              pip install pytest pytest-azurepipelines
              pytest -v --no-spark --no-postgresql --bigquery --napoleon-docstrings --junitxml=junit/test-results.xml --cov=. --cov-report=xml --cov-report=html --ignore=tests/cli --ignore=tests/integration/usage_statistics $(test_script)

            displayName: 'pytest'
            env:
              GOOGLE_APPLICATION_CREDENTIALS: $(gcp_authkey.secureFilePath)
              GE_TEST_BIGQUERY_PROJECT: $(GE_TEST_BIGQUERY_PROJECT)
              GE_TEST_BIGQUERY_DATASET: $(GE_TEST_BIGQUERY_DATASET)
